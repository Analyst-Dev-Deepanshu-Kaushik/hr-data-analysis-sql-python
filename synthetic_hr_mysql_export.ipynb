{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ff1bdb-ff0e-4a0a-83df-6d611b37a027",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: black;\">\n",
    "    <h1>üõ¢Ô∏è Export Synthetic HR Dataset to MySQL Database</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262db825-0d02-4014-90b7-bdffe872e368",
   "metadata": {},
   "source": [
    "### üõ°Ô∏è Step 1: Retrieve MySQL Password Securely\n",
    "To maintain security and avoid hardcoding sensitive credentials, we fetch the MySQL password from a system-level environment variable. This helps keep your notebook portable and safe for version control or collaboration. <br>\n",
    "If the password isn't set, an error message will be displayed prompting the user to verify their environment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "439fdfb0-6825-4213-8d16-fbf751137b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password retrieved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import the OS module to access environment variables\n",
    "import os\n",
    "# Retrieve the MySQL password stored as an environment variable\n",
    "mysql_password = os.getenv(\"MySQL_Password\")\n",
    "# Check whether the password was successfully retrieved\n",
    "if mysql_password is None:\n",
    "    # Display a warning if the environment variable is missing\n",
    "    print(\"Password not found in environment variables!\")\n",
    "else:\n",
    "    # Confirm successful retrieval (without printing the password itself)\n",
    "    print(\"Password retrieved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575fbc7-5640-43db-ba3c-2f3411ead9f9",
   "metadata": {},
   "source": [
    "### üß∞ Step 2: Import Libraries for Database Integration and Data Handling\n",
    "\n",
    "We import key Python libraries required for this data export workflow:\n",
    "- `pandas` and `numpy` for structured data manipulation\n",
    "- `mysql.connector` to establish a direct connection to the MySQL database\n",
    "\n",
    "These form the backbone of the ETL process as we move data from CSV files into relational tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3c0bbbc-ae88-4284-b518-0f6b1fd20a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for data manipulation and database connectivity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269407bf-61d4-49d8-bbc9-245fbba5d369",
   "metadata": {},
   "source": [
    "### üì§ Step 3: Export Attrition Records to MySQL\n",
    "\n",
    "In this step, we load the generated `attrition.csv` file and insert its contents into the `attrition` table within the `hr_synthetic` MySQL database. This dataset captures employee exit events, including exit date, reason, and satisfaction level.\n",
    "\n",
    "Before export, we:\n",
    "- Convert the exit date column to datetime format\n",
    "- Connect securely to the MySQL database using `mysql.connector`\n",
    "- Loop through each record and insert it into the `attrition` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8efd1d2-61da-4ca1-ba11-37be8e7bea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the attrition dataset from the synthetic data directory\n",
    "df_attrition = pd.read_csv(\"./hr_synthetic_data/attrition.csv\")\n",
    "\n",
    "# Convert 'exit_date' to datetime format for proper insertion into MySQL\n",
    "df_attrition[\"exit_date\"] = pd.to_datetime(df_attrition[\"exit_date\"])\n",
    "\n",
    "# Establish connection to the MySQL database using stored credentials\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Loop through each row and insert into the attrition table\n",
    "for _, row in df_attrition.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO attrition (employee_id, exit_date, reason, voluntary_exit, satisfaction_rating)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b54814-a72a-4c9e-894a-24a73e41a2fb",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Step 4: Export Department Reference Data to MySQL\n",
    "\n",
    "This step loads the static `departments.csv` reference file and inserts department records into the `departments` table. Each record includes:\n",
    "- A unique `department_id`\n",
    "- The corresponding `department_name`\n",
    "\n",
    "This table acts as a lookup for job roles and employee affiliations throughout the HR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cd0c85c-0aaa-4197-8fee-0e3c50fc3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the department reference dataset from CSV\n",
    "df_dept = pd.read_csv(\"./hr_synthetic_data/departments.csv\")\n",
    "\n",
    "# Establish connection to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Loop through each row and insert into the departments table\n",
    "for _, row in df_dept.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO departments (department_id, department_name)\n",
    "        VALUES (%s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Commit transaction and close connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd69df08-ad41-4c7e-9178-03e443abaf5b",
   "metadata": {},
   "source": [
    "### üë• Step 5: Export Employee Records to MySQL (Chunked Insertion)\n",
    "\n",
    "Given the large size of the `employees.csv` file, we process and insert data in manageable chunks (e.g. 5000 rows at a time). This improves memory efficiency and database performance.\n",
    "\n",
    "Each employee record includes:\n",
    "- Personal details (name, age, gender, marital status, etc.)\n",
    "- Employment metadata (job role, department, hire date, salary)\n",
    "- All dates are converted to proper datetime format before insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cdcb3d-3633-412c-ae77-49c4286e9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of rows to process at a time\n",
    "chunk_size = 5000\n",
    "\n",
    "# Establish connection to MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read the employee CSV in chunks for memory-efficient processing\n",
    "for chunk in pd.read_csv(\"./hr_synthetic_data/employees.csv\", chunksize=chunk_size):\n",
    "    \n",
    "    # Convert 'hire_date' to datetime for proper SQL formatting\n",
    "    chunk[\"hire_date\"] = pd.to_datetime(chunk[\"hire_date\"])\n",
    "    \n",
    "    # Insert each record into the employees table\n",
    "    for _, row in chunk.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO employees (\n",
    "                employee_id, name, age, gender, marital_status,\n",
    "                ethnicity, country, job_role_id, department_id, hire_date, salary\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\", tuple(row))\n",
    "\n",
    "    # Commit after each chunk is inserted\n",
    "    conn.commit()\n",
    "\n",
    "# Close cursor and connection to release resources\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013bf9ce-c866-487d-bed8-5becc3215b2d",
   "metadata": {},
   "source": [
    "### üìã Step 6: Export Hiring Source Data to MySQL\n",
    "\n",
    "This final export step loads the `hiring_sources.csv` file and inserts records into the `hiring_sources` table of the MySQL database. The dataset captures:\n",
    "\n",
    "- The source through which each employee was recruited\n",
    "- Referral information (if applicable)\n",
    "\n",
    "Before export, we sanitize the `referred_by` field by replacing blank and invalid entries with `None` to ensure compatibility with MySQL's NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4906cb4e-4203-49b6-a943-15b4610f5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hiring sources dataset from the synthetic data directory\n",
    "df_hiring = pd.read_csv(\"./hr_synthetic_data/hiring_sources.csv\")\n",
    "\n",
    "# Sanitize the 'referred_by' column by replacing blanks and invalid entries with None\n",
    "df_hiring[\"referred_by\"] = df_hiring[\"referred_by\"].replace([\"\", \"nan\", np.nan, pd.NA], None)\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert each row into the hiring_sources table\n",
    "for _, row in df_hiring.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO hiring_sources (employee_id, hiring_source, referred_by)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Finalize transaction and release database resources\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e0cc3-aae8-4710-afdb-e7db4074e63e",
   "metadata": {},
   "source": [
    "### üëî Step 7: Export Job Roles to MySQL\n",
    "\n",
    "This step loads the structured `job_roles.csv` file and inserts records into the `job_roles` table. Each job role is mapped to its parent department using a `department_id`, supporting organizational hierarchy and analytical joins.\n",
    "\n",
    "These role definitions are essential for tracking employee function and departmental composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553248ee-c855-4b9c-8041-674dce7d3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the job roles dataset from CSV\n",
    "df_roles = pd.read_csv(\"./hr_synthetic_data/job_roles.csv\")\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert each job role record into the job_roles table\n",
    "for _, row in df_roles.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO job_roles (job_role_id, job_role_name, department_id)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Finalize transaction and release database resources\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19faf4-4cca-43c2-8f0e-82ca60ac5b8b",
   "metadata": {},
   "source": [
    "### üìä Step 8: Export Performance Review Data to MySQL (Chunked Insertion)\n",
    "\n",
    "This section loads and inserts synthetic performance review records into the `performance_reviews` table. Each entry includes:\n",
    "\n",
    "- Annual review date\n",
    "- Performance score (1‚Äì5 scale)\n",
    "- Bonus eligibility indicator\n",
    "\n",
    "Chunked processing ensures the notebook remains memory-efficient when handling large volumes of employee evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e0bf511-ece1-4e2e-94d8-ec8c82dff6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of records to process per chunk\n",
    "chunk_size = 5000\n",
    "\n",
    "# Establish a connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read the performance review CSV in chunks\n",
    "for chunk in pd.read_csv(\"./hr_synthetic_data/performance_reviews.csv\", chunksize=chunk_size):\n",
    "    \n",
    "    # Convert 'review_date' to datetime for correct SQL formatting\n",
    "    chunk[\"review_date\"] = pd.to_datetime(chunk[\"review_date\"])\n",
    "    \n",
    "    # Insert each row into the performance_reviews table\n",
    "    for _, row in chunk.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO performance_reviews (\n",
    "                employee_id, review_date, performance_score, bonus_eligible\n",
    "            )\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", tuple(row))\n",
    "    \n",
    "    # Commit after processing each chunk\n",
    "    conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7024d64a-81d7-4306-9ed0-1eca31a016b3",
   "metadata": {},
   "source": [
    "### üéì Step 9: Export Promotion Records to MySQL\n",
    "\n",
    "This step loads the `promotions.csv` dataset and inserts promotion events into the `promotions` table of the MySQL database. Each record includes:\n",
    "\n",
    "- `employee_id`: the individual who was promoted\n",
    "- `promotion_date`: when the promotion occurred\n",
    "- `new_job_role_id`: the role assigned after promotion\n",
    "\n",
    "Promotion dates are converted to datetime format before insertion to ensure accurate SQL representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b646915-f757-43a9-bee6-4efe1fea16f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load promotion records from CSV\n",
    "df_promotions = pd.read_csv(\"./hr_synthetic_data/promotions.csv\")\n",
    "\n",
    "# Convert 'promotion_date' to datetime for proper SQL formatting\n",
    "df_promotions[\"promotion_date\"] = pd.to_datetime(df_promotions[\"promotion_date\"])\n",
    "\n",
    "# Establish connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert each promotion record into the promotions table\n",
    "for _, row in df_promotions.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO promotions (employee_id, promotion_date, new_job_role_id)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Commit transaction and release resources\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ad110-0716-4724-9244-108eee9e00d1",
   "metadata": {},
   "source": [
    "### üí∞ Step 10: Export Salary History to MySQL (Chunked Insertion)\n",
    "\n",
    "The final export step inserts employee salary progression data from `salaries.csv` into the `salaries` table. Each record includes:\n",
    "\n",
    "- Employee ID\n",
    "- Salary effective date (converted to datetime format)\n",
    "- Salary amount\n",
    "\n",
    "To ensure performance and scalability, the dataset is loaded and inserted in chunks (e.g. 5000 records at a time), making it suitable for large datasets and production-level workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede61d24-c8bc-439b-8b66-3c64a72e3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of records to load per chunk\n",
    "chunk_size = 5000\n",
    "\n",
    "# Establish a secure connection to the MySQL database\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=mysql_password,\n",
    "    database=\"hr_synthetic\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load the salaries CSV in chunks and insert into the database\n",
    "for chunk in pd.read_csv(\"./hr_synthetic_data/salaries.csv\", chunksize=chunk_size):\n",
    "    \n",
    "    # Convert 'effective_date' column to datetime format\n",
    "    chunk[\"effective_date\"] = pd.to_datetime(chunk[\"effective_date\"])\n",
    "    \n",
    "    # Insert each row into the salaries table\n",
    "    for _, row in chunk.iterrows():\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO salaries (\n",
    "                employee_id, effective_date, salary_amount\n",
    "            )\n",
    "            VALUES (%s, %s, %s)\n",
    "        \"\"\", tuple(row))\n",
    "    \n",
    "    # Commit after each chunk is inserted\n",
    "    conn.commit()\n",
    "\n",
    "# Close database connection\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23c104-d234-4d0b-a216-ce8d36853129",
   "metadata": {},
   "source": [
    "### ‚úÖ Final Summary\n",
    "\n",
    "In this notebook, we successfully exported a comprehensive synthetic HR dataset from structured CSV files into a normalized MySQL database. Each table reflects real-world employee lifecycle metadata across 10 dimensions:\n",
    "\n",
    "- üè¢ Departments and Job Roles\n",
    "- üë• Employee Demographics and Metadata\n",
    "- üí∞ Salary Progression Over Time\n",
    "- üìà Performance Reviews and Bonus Eligibility\n",
    "- üéì Promotion Events\n",
    "- üö™ Attrition Events and Exit Insights\n",
    "- üîç Hiring Sources and Referral Information\n",
    "\n",
    "üîß Key Engineering Highlights:\n",
    "- Passwords securely retrieved via environment variables\n",
    "- Datetime formatting applied consistently before database insertion\n",
    "- Efficient chunk-based loading for large datasets\n",
    "- Clear table separation aligned with analytical and relational schemas\n",
    "\n",
    "üéØ Use Cases:\n",
    "- Power BI / Tableau dashboards for HR metrics\n",
    "- Machine learning pipelines (attrition prediction, salary modeling)\n",
    "- Full-stack app development with realistic sample data\n",
    "- SQL practice and schema optimization for career growth\n",
    "\n",
    "This notebook is designed for modularity, security, and clarity ‚Äî ideal for personal portfolios, employer showcase, or client delivery. Combine this with the data generator notebook for a complete synthetic HR data pipeline üîÅ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812a610-8083-4e34-8d42-a40933ecf56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
